
[toc]

---

# CS基础

## 计算机组成原理 

## 操作系统 

## 数据结构和算法

## 计算机网络

## 编程语言

### C

### python

---
# 网络空间安全

## 系统安全

## web安全

## 移动安全

## 企业安全
---
# AI

## 特征工程

## 常见算法及模型
### 机器学习
#### KNN
#### Naive Bayes
#### Decision Tree
#### Random Forest
#### SVM

### 深度学习
#### CNN
#### RNN

### 其他算法
#### PCA
#### K-means 

## 常见问题

### 损失函数、代价函数、目标函数 
__首先给出结论__  
- `损失函数`和`代价函数`是同一个东西
- `目标函数`是一个与他们相关但更广的概念，对于目标函数来说在**有约束条件下的最小化**就是`损失函数(loss function)`  
![image](https://pic3.zhimg.com/v2-3f4959cd70308df496ecc4568a0d982d_r.jpg)

上面三个图的函数依次为`$f_1(x)$`, `$f_2(x)$`, `$f_3(x)$`。  
我们是想用这三个函数分别来拟合Price，Price的真实值记为`$Y$`。  

我们给定`$x$`，这三个函数都会输出一个`$f_(x)$`,这个输出的`$f_(x)$`与真实值`$Y$`可能是相同的，也可能是不同的，为了表示我们拟合的好坏，我们就用一个函数来**度量拟合的程度**，比如：`$L(Y, f(X))= (Y-f(X))^2$`，这个函数就称为`损失函数(loss function)`，或者叫`代价函数(cost function)`。  

损失函数越小，就代表模型拟合的越好。那是不是我们的目标就只是让loss function越小越好呢？还不是。
这个时候还有一个概念叫风险函数(risk function)。  

`风险函数`是**损失函数的期望**，这是由于我们输入输出的`$(X,Y)$`遵循一个联合分布，但是这个联合分布是未知的，所以无法计算。但是我们是有历史数据的，就是我们的训练集，`$f(X)$`关于训练集的平均损失称作经验风险(empirical risk)，即`$\frac{1}{N}\sum_{i=1}^\infty L(y_i,f(x_i))$`，所以我们的目标就是最小化`$\frac{1}{N}\sum_{i=1}^\infty L(y_i,f(x_i))$`，称为**经验风险最小化**。

如果到这一步就完了的话，那我们看上面的图，那肯定是最右面的`$f_3(x)$`的经验风险函数最小了，因为它对历史的数据拟合的最好嘛。但是我们从图上来看`$f_3(x)$`肯定不是最好的，因为它过度学习历史数据，导致它在真正预测时效果会很不好，这种情况称为**过拟合(over-fitting)**。  

它的函数太复杂了，都有四次方了，这就引出了下面的概念，我们不仅要让经验风险最小化，还要让结构风险最小化。这个时候就定义了一个函数`$J(f)$` ，这个函数专门用来度量模型的复杂度，在机器学习中也叫`正则化(regularization)` 。常用的有`$L1$`, `$L2$` 范数。  

到这一步我们就可以说我们最终的优化函数是：`$min\frac{1}{N}\sum_{i=1}^\infty L(y_i,f(x_i))+\lambda J(f)$` ，即最优化**经验风险**和**结构风险**，而这个函数就被称为`目标函数`。  

结合上面的例子来分析：最左面的`$f_1(x)$`结构风险最小（模型结构最简单），但是经验风险最大（对历史数据拟合的最差）；最右面的`$f_3(x)$`经验风险最小（对历史数据拟合的最好），但是结构风险最大（模型结构最复杂）;而`$f_2(x)$`达到了二者的良好平衡，最适合用来预测未知数据集。  

forward by https://www.zhihu.com/question/52398145

---
### 欠拟合和过拟合

> 欠拟合和过拟合在机器学习中经常会出现，这里总结一下出现欠拟合和过拟合的原因、情形和解决方法。

**欠拟合**  


---
<span id="梯度消失和梯度爆炸">  

### 梯度消失和梯度爆炸  
> 梯度消失的根源是：深度神经网络和反向传播  

**反向传播**：目前神经网络的优化方法大都基于反向传播的思想，即根据loss函数的误差通过梯度反向传播的方式指导深度神经网络的优化。

**梯度消失**：一般导致梯度消失的原因是在`深层网络`中使用了sigmoid等不合适的`激活函数`。  
例如：在4层的全连接网络中，第`$i$`层网络激活(`$f$`为激活函数)后的输出为`$f_i(x)$`，则`$f_{i+1}=f(f_i*w_{i+1}+b_{i+1})$`，基于梯度下降的优化策略，假设学习率为`$\alpha$`，得到参数更新为`$\Delta w=-\alpha \frac {\partial Loss}{\partial w}$`，根据链式求导法则，得到其上一层中更新梯度`$\Delta w_2=-\alpha \frac {\partial Loss}{\partial w_2}=-\alpha \frac {\partial Loss}{\partial f_4}\frac {\partial f_4}{\partial f_3}\frac {\partial f_3}{\partial f_2}\frac {\partial f_2}{\partial w_2}$`，其中`$\frac {\partial f_4}{\partial f_3}$`就是对激活函数的求导。重点来了，如果这个值`<1`，那么随着层数的增加，经过很多次乘积后，就会导致其值无限趋近于0，即梯度消失。  

**梯度爆炸**：原理同上，当激活函数求导后的值`>1`时，随着层数的增加，经过很多次乘积后，就会导致其值无限大，即梯度爆炸。

**激活函数**：根据上面的原理，当激活为sigmoid时，其倒数是恒<=0.25的，所以当网络层数很多时，很容易导致梯度消失。sigmoid的函数为：`$sigmoid(x)=\frac {1}{1+e^{-x}}$`  
另外，激活函数`$tanh(x)$`的导数恒<=1，也存在梯度消失问题。

**解决方案**：
- 使用Relu、leakRelu、elu等激活函数
- batchnorm(batch normalization)
- 残差结构
- LSTM
- 预训练加微调
- 梯度剪枝、权重正则（仅针对梯度爆炸）

---

# AI与安全

## 对抗
### 使用AI攻击目标系统
### 攻击AI系统
#### 攻击AI框架
#### 攻击AI模型
##### 对抗样本

## 防御
### 使用AI保护目标系统
#### 恶意软件检测
#### web安全检测
##### CH网络异常流量检测
### 保护AI系统
