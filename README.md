
[toc]

---

**写在最前面**  
github的markdown不支持LaTeX，所以公式没法显示出来。有需求可自行下载源文件到支持LaTeX的编辑器中看，效果更佳

---

# CS基础

## 计算机组成原理 

## 操作系统 

## 数据结构和算法

## 计算机网络

## 编程语言

### C

### python

---
# 网络空间安全

## 系统安全

## web安全

## 移动安全

## 企业安全
---
# AI

## 特征工程

## 常见算法及模型
### 机器学习
#### KNN
#### Naive Bayes
#### Decision Tree
#### Random Forest
#### SVM


### 深度学习
#### CNN
#### RNN
循环神经网络 Recurrent Neural Network
> 下图介绍了RNN的基本原理，图片来源于台大李宏毅教授PPT  

![RNN](https://pic4.zhimg.com/80/v2-f716c816d46792b867a6815c278f11cb_hd.jpg)  

`$x$`为当前状态下的输入  
`$h$`为收到的上一节点的输入。其中主要保存了先前节点的记忆数据  
`$y$`为当前节点的输出。通常使用`$\dot{h}$`进行映射到一个线性层，再用softmax进行分类得到结果  
`$\dot {h}$`为当前节点传给下一节点的记忆。  

__单个节点计算方式如上图，连起来就形成了RNN序列__  
![RNN](https://pic2.zhimg.com/80/v2-71652d6a1eee9def631c18ea5e3c7605_hd.jpg)  

---

##### LSTM
> Long short-term memory， 长短时记忆，RNN变种，主要可以进行选择性记忆，并且解决了一般深度神经网络中梯度消失和梯度爆炸的问题。  

![LSTM](https://pic4.zhimg.com/80/v2-e4f9851cad426dfe4ab1c76209546827_hd.jpg)  
上图把LSTM与原始RNN进行了对比。  

LSTM vs RNN：  
上图可以看出naive RNN和LSTM最大的不同在于LSTM每次都传递两个值，`$c^t$`和`$h^t$`。其中，`$c^t$`主要用于存储历史节点的记忆信息，相当于naive RNN中的`$h^t$`（因为naive RNN只有一个`$h^t$`），他们都包含了以往节点中的`记忆数据`，属于重要的信息，每次传递都会加入当前节点的内容，变化相对来说较小；而LSTM的另一个值`$h^t$`（不同于naive RNN的`$h^t$`）则主要是为了和当前输入进行运算来获得`门控信号`，本身不含记忆数据，且每次传递时因为主要由当前的输入决定，变化来说相对较大。

LSTM中`$c^t$`和`$h^t$`的计算如下图所示：  
![LSTM](https://pic2.zhimg.com/80/v2-556c74f0e025a47fea05dc0f76ea775d_hd.jpg)  

明显可以看出，LSTM中，`$c^t$`主要包含了记忆数据，`$h^t$`主要用于计算门控信号。  

_Tips:_  
`$z$`表示处理过的当前信息  
`$z^f$`、`$z^i$`、`$z^o$`: 均为使用当前输入`$x^t$`和上个节点传递的`$h^{t-1}$`进行计算得到的门控信号。其中f为forget，表遗忘门控；i为information，表记忆门控；o为output，表输出门控。

```math
z^f = \sigma(W^fX^t+b^f)  

z^i = \sigma(W^iX^t+b^i)  

z^o = \sigma(W^oX^t+b^o)  
```
其中`$\sigma$`函数代表sigmoid函数

LSTM的效果：  
1、当gate是关闭的，那么就会阻止对当前信息的改变，这样以前的依赖信息就会被学到。  
2、当gate是打开的时候，并不是完全替换之前的信息，而是在之前信息和现在信息之间做加权平均。所以，无论网络的深度有多深，输入序列有多长，只要gate是打开的，网络都会记住这些信息。

**LSTM如何解决梯度消失/爆炸**：   
<a href="#梯度消失和梯度爆炸">什么是梯度消失和梯度爆炸</a>  

LSTM使用门控有选择的让一部分信息通过。门控单元是由一个sigmoid单元和一个逐点乘积操作组成，sigmoid单元输出1或0，用来判断通过还是阻止，然后训练这些gate的组合。所以，当gate是打开的（梯度接近于1），梯度就不会vanish。并且sigmoid不超过1，那么梯度也不会explode。

简明数学分析：  
为了便于分析，如果考虑bias，同时忽略输入变量`$h_{j-1}$`的作用，那么隐含层之间的关系可以表示为:  
```math
c_j = \sigma(W^fX_j+b^f)c_{j-1} + \sigma(W^iX_j+b^i)\sigma(WX_j+b)
```
需要连乘的项为：
```math
\frac {\partial c_j}{c_{j-1}} = \sigma(W^fX_j+b)
```  
该值范围在0~1之间，但是在实际参数更新中，可以通过控制bias比较大，使得该值接近于1；在这种情况下，即使通过很多次连乘的操作，梯度也不会消失，仍然可以保留"长距"连乘项的存在。即总可以通过选择合适的参数，在不发生梯度爆炸的情况下，找到合理的梯度方向来更新参数，而且这个方向可以充分地考虑远距离的隐含层信息的传播影响。

---

### 其他算法
#### PCA
#### K-means 

## 常见问题

### 损失函数、代价函数、目标函数 
__首先给出结论__  
- `损失函数`和`代价函数`是同一个东西
- `目标函数`是一个与他们相关但更广的概念，对于目标函数来说在**有约束条件下的最小化**就是`损失函数(loss function)`  
![image](https://pic3.zhimg.com/v2-3f4959cd70308df496ecc4568a0d982d_r.jpg)

上面三个图的函数依次为`$f_1(x)$`, `$f_2(x)$`, `$f_3(x)$`。  
我们是想用这三个函数分别来拟合Price，Price的真实值记为`$Y$`。  

我们给定`$x$`，这三个函数都会输出一个`$f_(x)$`,这个输出的`$f_(x)$`与真实值`$Y$`可能是相同的，也可能是不同的，为了表示我们拟合的好坏，我们就用一个函数来**度量拟合的程度**，比如：`$L(Y, f(X))= (Y-f(X))^2$`，这个函数就称为`损失函数(loss function)`，或者叫`代价函数(cost function)`。  

损失函数越小，就代表模型拟合的越好。那是不是我们的目标就只是让loss function越小越好呢？还不是。
这个时候还有一个概念叫风险函数(risk function)。  

`风险函数`是**损失函数的期望**，这是由于我们输入输出的`$(X,Y)$`遵循一个联合分布，但是这个联合分布是未知的，所以无法计算。但是我们是有历史数据的，就是我们的训练集，`$f(X)$`关于训练集的平均损失称作经验风险(empirical risk)，即`$\frac{1}{N}\sum_{i=1}^\infty L(y_i,f(x_i))$`，所以我们的目标就是最小化`$\frac{1}{N}\sum_{i=1}^\infty L(y_i,f(x_i))$`，称为**经验风险最小化**。

如果到这一步就完了的话，那我们看上面的图，那肯定是最右面的`$f_3(x)$`的经验风险函数最小了，因为它对历史的数据拟合的最好嘛。但是我们从图上来看`$f_3(x)$`肯定不是最好的，因为它过度学习历史数据，导致它在真正预测时效果会很不好，这种情况称为**过拟合(over-fitting)**。  

它的函数太复杂了，都有四次方了，这就引出了下面的概念，我们不仅要让经验风险最小化，还要让结构风险最小化。这个时候就定义了一个函数`$J(f)$` ，这个函数专门用来度量模型的复杂度，在机器学习中也叫`正则化(regularization)` 。常用的有`$L1$`, `$L2$` 范数。  

到这一步我们就可以说我们最终的优化函数是：`$min\frac{1}{N}\sum_{i=1}^\infty L(y_i,f(x_i))+\lambda J(f)$` ，即最优化**经验风险**和**结构风险**，而这个函数就被称为`目标函数`。  

结合上面的例子来分析：最左面的`$f_1(x)$`结构风险最小（模型结构最简单），但是经验风险最大（对历史数据拟合的最差）；最右面的`$f_3(x)$`经验风险最小（对历史数据拟合的最好），但是结构风险最大（模型结构最复杂）;而`$f_2(x)$`达到了二者的良好平衡，最适合用来预测未知数据集。  

forward by https://www.zhihu.com/question/52398145

---
### 欠拟合和过拟合

> 欠拟合和过拟合在机器学习中经常会出现，这里总结一下出现欠拟合和过拟合的原因、情形和解决方法。

**欠拟合**  


---
<span id="梯度消失和梯度爆炸">  

### 梯度消失和梯度爆炸  
> 梯度消失的根源是：深度神经网络和反向传播  

**反向传播**：目前神经网络的优化方法大都基于反向传播的思想，即根据loss函数的误差通过梯度反向传播的方式指导深度神经网络的优化。

**梯度消失**：一般导致梯度消失的原因是在`深层网络`中使用了sigmoid等不合适的`激活函数`。  
例如：在4层的全连接网络中，第`$i$`层网络激活(`$f$`为激活函数)后的输出为`$f_i(x)$`，则`$f_{i+1}=f(f_i*w_{i+1}+b_{i+1})$`，基于梯度下降的优化策略，假设学习率为`$\alpha$`，得到参数更新为`$\Delta w=-\alpha \frac {\partial Loss}{\partial w}$`，根据链式求导法则，得到其上一层中更新梯度`$\Delta w_2=-\alpha \frac {\partial Loss}{\partial w_2}=-\alpha \frac {\partial Loss}{\partial f_4}\frac {\partial f_4}{\partial f_3}\frac {\partial f_3}{\partial f_2}\frac {\partial f_2}{\partial w_2}$`，其中`$\frac {\partial f_4}{\partial f_3}$`就是对激活函数的求导。重点来了，如果这个值`<1`，那么随着层数的增加，经过很多次乘积后，就会导致其值无限趋近于0，即梯度消失。  

**梯度爆炸**：原理同上，当激活函数求导后的值`>1`时，随着层数的增加，经过很多次乘积后，就会导致其值无限大，即梯度爆炸。

**激活函数**：根据上面的原理，当激活为sigmoid时，其倒数是恒<=0.25的，所以当网络层数很多时，很容易导致梯度消失。sigmoid的函数为：`$sigmoid(x)=\frac {1}{1+e^{-x}}$`  
另外，激活函数`$tanh(x)$`的导数恒<=1，也存在梯度消失问题。

**解决方案**：
- 使用Relu、leakRelu、elu等激活函数
- batchnorm(batch normalization)
- 残差结构
- LSTM
- 预训练加微调
- 梯度剪枝、权重正则（仅针对梯度爆炸）

---

# AI与安全

## 对抗
### 使用AI攻击目标系统
### 攻击AI系统
#### 攻击AI框架
#### 攻击AI模型
##### 对抗样本

## 防御
### 使用AI保护目标系统
#### 恶意软件检测
#### web安全检测
##### CH网络异常流量检测
### 保护AI系统
